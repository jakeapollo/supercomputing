{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe just a bunch of ANDs in every layer, then ANDs of ANDs? Issue: sparsity goes down.\n",
    "\n",
    "Just replace ANDs with at-least-two-of-a-small-subset functions? Say each input node is on with prob\n",
    "$p$; consider an output node with $k$ inputs; the prob this has two inputs on is $p^2(1-p)^{k-2}\n",
    "\\binom{k}{2}$. The prob it has at least two inputs on is $1-(1-p)^k-kp (1-p)^{k-1}$. The difference\n",
    "between these two is $O\\left(\\frac{(pk)^3}{1-pk}\\right)$, from summing a geometric series. Setting\n",
    "the first one to $p$ gives $p^2(1-p)^{k-2} \\binom{k}{2}=p$, which implies\n",
    "$p(1-p)^{k-2}=\\frac{2}{k(k-1)}$. This should work out at about $p=\\frac{2}{(k-1)^2}$, up to\n",
    "lower-order corrections, which also makes the diff small. Equivalently, $k=\\sqrt{\\frac{2}{p}}+1$ We\n",
    "might want to correct the ideal network such that it is more precisely binary though? I.e., we might\n",
    "want to do it without ReLUs? I guess can try both options, but let's first try the one without ReLUs\n",
    "that just computes these gates with perfect binary outputs.\n",
    "\n",
    "\n",
    "Here's an alternative calculation in a slightly different setup (though probably they are the same\n",
    "up to error terms in some reasonable sense). Let's say each entry of the weight matrix Bernoulli\n",
    "with probability $q$ and each input is Bernoulli with probability $p$. We want it to be the case\n",
    "that taking a random matrix and a random input, the probability that an output is on is $p$. The\n",
    "probability it is on is the probability that there are at least two simultaneous hits from that row\n",
    "of the weight matrix and the input. Each hit has probability $pq$, so this has probability\n",
    "$1-(1-pq)^m-m pq (1-pq)^{m-1}$. So to keep sparsity constant, we want $1-(1-pq)^m-m pq\n",
    "(1-pq)^{m-1}=p$. Up to sth like a $O((mpq)^3/(1-mpq))$ term as before, we can just solve $p=(pq)^2\n",
    "m(m-1)/2$. This gives $q=\\sqrt{\\frac{2}{m(m-1)p}}$. A more precise solution can be found using\n",
    "numerical methods (after all, fixing $m$ and $p$, it's just a matter of finding a root of a\n",
    "polynomial in $q$), I think. But this should be fine for us for now.\n",
    "\n",
    "Jake: roughly we want $p=(qp)^2 m\\implies q=\\frac 1 {m\\sqrt p}$. If we decide to pick $p=q$, we have\n",
    "$p=m^{1/3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "m = 1000  # dim of ideal sparse net\n",
    "p = 1 / math.sqrt(m)  # prob each input is on; should also be the prob each gate later on is on\n",
    "q = math.sqrt(2 / (m * (m - 1) * p))  # prob each weight matrix entry is 1\n",
    "\n",
    "# k = math.sqrt(2/p)+1 # fan-in\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Custom activation function\n",
    "def custom_activation(x):\n",
    "    return F.relu(x) - F.relu(x - 1)\n",
    "\n",
    "\n",
    "# Custom layer with specified properties\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, probability_q):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.probability_q = probability_q\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weights.data = torch.bernoulli(\n",
    "            torch.full((self.output_dim, self.input_dim), self.probability_q)\n",
    "        )\n",
    "        self.bias.data.fill_(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return custom_activation(F.linear(x, self.weights, self.bias))\n",
    "\n",
    "\n",
    "class SummationLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SummationLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Initialize coefficients as 1 or -1 with 50/50 probability\n",
    "        self.coefficients = torch.where(\n",
    "            torch.rand(input_dim) > 0.5, torch.tensor(1.0), torch.tensor(-1.0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust for the case where x might not be batched\n",
    "        if x.dim() == 1:\n",
    "            # x is a 1D tensor, implying a single sample rather than a batch\n",
    "            return torch.sum(x * self.coefficients, dim=0, keepdim=True)\n",
    "        else:\n",
    "            # x is a 2D tensor, implying a batch of samples\n",
    "            return torch.sum(x * self.coefficients, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "# Neural Network with L Custom Layers and a Summation Layer at the end\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, layer_dims, probability_q):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(CustomLayer(layer_dims[i - 1], layer_dims[i], probability_q))\n",
    "\n",
    "        # Add the summation layer at the end, treated as just another layer\n",
    "        self.summation_layer = SummationLayer(layer_dims[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [\n",
    "            x\n",
    "        ]  # List to store activations from each layer, including the input and output layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            activations.append(x)  # Store the activation of each layer\n",
    "        # Apply the summation layer and treat its output as the activation of the final layer\n",
    "        x = self.summation_layer(x)\n",
    "        activations.append(x)  # Include the final output as the last \"activation\"\n",
    "        return activations  # Now, this returns a list of activations for all layers, including the final layer\n",
    "\n",
    "\n",
    "# Example usage\n",
    "L = 10  # num of layers, including input but not the 1-neuron output\n",
    "layer_dims = [m] * L  # Dimension of each layer including input and output dimension\n",
    "probability_q = q  # Probability of presence of each entry in weight matrix\n",
    "\n",
    "# Initialize the network\n",
    "ideal_network = CustomNetwork(layer_dims, probability_q)\n",
    "\n",
    "# Assuming x is your input tensor\n",
    "# x = torch.randn(batch_size, layer_dims[0])  # Example input; adjust the size accordingly\n",
    "# activations = net(x)\n",
    "# `activations` is a list of activations from each layer, including the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # dim into which we'll try to compress the ideal net\n",
    "\n",
    "# creating the embedding from U_1 to V_1\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_matrix_with_unit_norm_columns(n, m):\n",
    "    \"\"\"\n",
    "    Create an n x m matrix E where each column has a unit norm.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of rows in E, corresponding to the dimension of V_1.\n",
    "        m (int): Number of columns in E, corresponding to the dimension of U_1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The matrix E with each column normalized to have a unit norm.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate an n x m matrix with Gaussian entries\n",
    "    E = torch.randn(n, m)\n",
    "\n",
    "    # Step 2: Normalize each column to have a unit norm\n",
    "    norms = torch.norm(E, dim=0, keepdim=True)\n",
    "    E_normalized = E / norms\n",
    "\n",
    "    return E_normalized\n",
    "\n",
    "\n",
    "# Example dimensions\n",
    "# n = 100  # Dimension for the small network input (V_1)\n",
    "# m = 200  # Dimension for the big network input (U_1)\n",
    "\n",
    "# Generate the matrix E\n",
    "E = create_matrix_with_unit_norm_columns(n, m)\n",
    "\n",
    "print(f\"Shape of E: {E.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supercompute-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
