{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe just a bunch of ANDs in every layer, then ANDs of ANDs? Issue: sparsity goes down.\n",
    "\n",
    "Just replace ANDs with at-least-two-of-a-small-subset functions? Say each input node is on with prob\n",
    "$p$; consider an output node with $k$ inputs; the prob this has two inputs on is $p^2(1-p)^{k-2}\n",
    "\\binom{k}{2}$. The prob it has at least two inputs on is $1-(1-p)^k-kp (1-p)^{k-1}$. The difference\n",
    "between these two is $O\\left(\\frac{(pk)^3}{1-pk}\\right)$, from summing a geometric series. Setting\n",
    "the first one to $p$ gives $p^2(1-p)^{k-2} \\binom{k}{2}=p$, which implies\n",
    "$p(1-p)^{k-2}=\\frac{2}{k(k-1)}$. This should work out at about $p=\\frac{2}{(k-1)^2}$, up to\n",
    "lower-order corrections, which also makes the diff small. Equivalently, $k=\\sqrt{\\frac{2}{p}}+1$ We\n",
    "might want to correct the ideal network such that it is more precisely binary though? I.e., we might\n",
    "want to do it without ReLUs? I guess can try both options, but let's first try the one without ReLUs\n",
    "that just computes these gates with perfect binary outputs.\n",
    "\n",
    "\n",
    "Here's an alternative calculation in a slightly different setup (though probably they are the same\n",
    "up to error terms in some reasonable sense). Let's say each entry of the weight matrix Bernoulli\n",
    "with probability $q$ and each input is Bernoulli with probability $p$. We want it to be the case\n",
    "that taking a random matrix and a random input, the probability that an output is on is $p$. The\n",
    "probability it is on is the probability that there are at least two simultaneous hits from that row\n",
    "of the weight matrix and the input. Each hit has probability $pq$, so this has probability\n",
    "$1-(1-pq)^m-m pq (1-pq)^{m-1}$. So to keep sparsity constant, we want $1-(1-pq)^m-m pq\n",
    "(1-pq)^{m-1}=p$. Up to sth like a $O((mpq)^3/(1-mpq))$ term as before, we can just solve $p=(pq)^2\n",
    "m(m-1)/2$. This gives $q=\\sqrt{\\frac{2}{m(m-1)p}}$. A more precise solution can be found using\n",
    "numerical methods (after all, fixing $m$ and $p$, it's just a matter of finding a root of a\n",
    "polynomial in $q$), I think. But this should be fine for us for now.\n",
    "\n",
    "Jake: roughly we want $p=(qp)^2 m\\implies q=\\frac 1 {m\\sqrt p}$. If we decide to pick $p=q$, we have\n",
    "$p=m^{1/3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(x):\n",
    "    return F.relu(x) - F.relu(x - 1)\n",
    "\n",
    "\n",
    "# Custom layer with specified properties\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, probability_q, p_positive=0.5, bias_value=-1):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.probability_q = probability_q\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        # self.weights = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        # self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.layer = nn.Linear(input_dim, output_dim)\n",
    "        self.bias_value = bias_value\n",
    "        self.p_positive = p_positive\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer.weight.data = torch.bernoulli(\n",
    "            torch.full((self.output_dim, self.input_dim), self.probability_q)\n",
    "        )\n",
    "        random_signs = (\n",
    "            torch.bernoulli(self.p_positive * torch.ones(self.output_dim, self.input_dim)) * 2 - 1\n",
    "        )\n",
    "        self.layer.weight.data = self.layer.weight.data * random_signs\n",
    "        self.layer.bias.data.fill_(self.bias_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return custom_activation(x)\n",
    "\n",
    "\n",
    "class SummationLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SummationLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Initialize coefficients as 1 or -1 with 50/50 probability\n",
    "        self.coefficients = torch.where(\n",
    "            torch.rand(input_dim) > 0.5, torch.tensor(1.0), torch.tensor(-1.0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust for the case where x might not be batched\n",
    "        if x.dim() == 1:\n",
    "            # x is a 1D tensor, implying a single sample rather than a batch\n",
    "            return torch.sum(x * self.coefficients, dim=0, keepdim=True)\n",
    "        else:\n",
    "            # x is a 2D tensor, implying a batch of samples\n",
    "            return torch.sum(x * self.coefficients, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "# Neural Network with L Custom Layers and a Summation Layer at the end\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, layer_dims, probability_q, p_positive=0.5, bias_value=-1, final_summation=False\n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(\n",
    "                CustomLayer(layer_dims[i - 1], layer_dims[i], probability_q, p_positive, bias_value)\n",
    "            )\n",
    "\n",
    "        # Add the summation layer at the end, treated as just another layer\n",
    "        self.final_summation = final_summation\n",
    "        if final_summation:\n",
    "            self.summation_layer = SummationLayer(layer_dims[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # List to store activations from each layer, including the input and output layers\n",
    "        activations = [x]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            activations.append(x)  # Store the activation of each layer\n",
    "        # Apply the summation layer and treat its output as the activation of the final layer\n",
    "        if self.final_summation:\n",
    "            x = self.summation_layer(x)\n",
    "            activations.append(x)  # Include the final output as the last \"activation\"\n",
    "        return x, activations  # Return a list of activations for all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_with_unit_norm_columns(d, m):\n",
    "    \"\"\"\n",
    "    Create an d x m matrix E where each column has a unit norm.\n",
    "\n",
    "    Args:\n",
    "        d (int): Number of rows in W_E, corresponding to the dimension of V_1.\n",
    "        m (int): Number of columns in W_E, corresponding to the dimension of U_1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The matrix W_E with each column normalized to have a unit norm.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate an n x m matrix with Gaussian entries\n",
    "    W_E = torch.randn(d, m)\n",
    "\n",
    "    # Step 2: Normalize each column to have a unit norm\n",
    "    norms = torch.norm(W_E, dim=0, keepdim=True)\n",
    "    W_E_normalized = W_E / norms\n",
    "\n",
    "    return W_E_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many inputs do we need to give the small net a reasonable chance to learn the ideal algorithm? Well, for a lower bound, the number of times each output gate is active with one of the inputs being active should be at least one in expectation, I guess. But well, maybe this is a bad question to ask. We should probably just keep training on random inputs and track the loss over time. It'll plausibly be clear from the loss curve if the algo is still getting learned or if we've peaked. Should maybe still hold out a small number of inputs as a test set though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdealNetworkDataset(Dataset):\n",
    "    def __init__(self, length, m, p, W_E, ideal_network):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            length (int): Number of items in the dataset.\n",
    "            m (int): Dimension of the input vectors for the big network (U_1).\n",
    "            p (float): Probability of an entry being 1 in the input vector.\n",
    "            E (torch.Tensor): Matrix for mapping u to v (dimensions n x m).\n",
    "            ideal_network (CustomNetwork): The ideal (big) network to run input through and\n",
    "            get activations.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self.m = m\n",
    "        self.p = p\n",
    "        self.W_E = W_E\n",
    "        self.ideal_network = ideal_network\n",
    "        self.ideal_inputs = torch.bernoulli(torch.full((self.m, length), self.p))\n",
    "        self.compressed_inputs = self.W_E @ self.ideal_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a random vector u in U_1\n",
    "        ideal_input = self.ideal_inputs[:, idx]\n",
    "        compressed_input = self.compressed_inputs[:, idx]\n",
    "\n",
    "        # Run u (not v) through the ideal network and store activations at each layer\n",
    "        _, ideal_activations = self.ideal_network(ideal_input)\n",
    "\n",
    "        # Return the input for the small network (v) and the corresponding activations as targets\n",
    "        return (compressed_input, ideal_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNetwork(nn.Module):\n",
    "    def __init__(self, layer_dims, readoff_dims, nonlinearity=F.relu, final_summation=False):\n",
    "        \"\"\"\n",
    "        Initializes the small network with specific layer dimensions and readoff layers for all\n",
    "        activations, including a separate readoff for the summation layer's output.\n",
    "\n",
    "        Args:\n",
    "        layer_dims (list): Dimensions of the small network's layers, including the input layer.\n",
    "            Typically [d]*L, where d is the dimension of the compressed input and L is the number of\n",
    "            layers.\n",
    "        readoff_dims (list): Target dimensions for the readoff layers, corresponding to each\n",
    "            layer in the big network. Typically [m]*L, where m is the dimension of the input to\n",
    "            thebig network and L is the number of layers.\n",
    "        \"\"\"\n",
    "        super(SmallNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.readoff_layers = nn.ModuleList()\n",
    "\n",
    "        # Initialize the small network layers with ReLU activations\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(nn.Linear(layer_dims[i - 1], layer_dims[i]))\n",
    "\n",
    "        # Final summation layer\n",
    "        self.final_summation = final_summation\n",
    "        if final_summation:\n",
    "            self.summation_layer = SummationLayer(layer_dims[-1])\n",
    "\n",
    "        # Ensure we have a readoff layer for each layer in layer_dims plus one for the summation\n",
    "        # layer\n",
    "        for i, dim in enumerate(layer_dims):\n",
    "            self.readoff_layers.append(nn.Linear(dim, readoff_dims[i], bias=False))\n",
    "        if final_summation:\n",
    "            self.readoff_layers.append(nn.Linear(1, 1, bias=False))\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [x]  # Store activations from the small network, including input\n",
    "        for layer in self.layers:\n",
    "            x = self.nonlinearity(layer(x))\n",
    "            activations.append(x)\n",
    "\n",
    "        # Apply the summation layer to the output of the last linear layer\n",
    "        if self.final_summation:\n",
    "            summation_output = self.summation_layer(activations[-1])\n",
    "            activations.append(summation_output)\n",
    "\n",
    "        # Map activations and summation output through their respective readoff layers\n",
    "        readoff_activations = [\n",
    "            self.readoff_layers[i](act.unsqueeze(0)) for i, act in enumerate(activations)\n",
    "        ]\n",
    "\n",
    "        return readoff_activations  # Adjust return types as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000  # dim of ideal sparse net\n",
    "# prob each input is on; should also be the prob each gate later on is on\n",
    "p = 1 / 100  # math.log(m) * m ** (-1)\n",
    "q = 0.1  # (2 / (m * (m - 1) * p)) ** (0.1)  # prob each weight matrix entry is nonzero\n",
    "print(p, q)\n",
    "b = -1\n",
    "n = 100  # dim into which we'll try to compress the ideal net\n",
    "batch_size = 10000\n",
    "dataset_length = batch_size  # Number of data points\n",
    "L = 20  # num of layers, including input but not the 1-neuron output\n",
    "layer_dims = [m] * L  # Dimension of each layer including input and output dimension\n",
    "probability_q = q  # Probability of presence of each entry in weight matrix\n",
    "# k = math.sqrt(2/p)+1 # fan-in\n",
    "\n",
    "p_positive = 0.25  # Probability of a weight being positive\n",
    "ideal_network = CustomNetwork(layer_dims, probability_q, p_positive, bias_value=b)\n",
    "\n",
    "# Initialize the dataset\n",
    "W_E = create_matrix_with_unit_norm_columns(n, m)\n",
    "\n",
    "ideal_network_dataset = IdealNetworkDataset(dataset_length, m, p, W_E, ideal_network)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(ideal_network_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "layer_dims = [n] * L  # Layer dimensions for the small network, excluding the summation layer\n",
    "readoff_dims = [m] * L  # Readoff dimensions for each layer plus the summation layer\n",
    "\n",
    "small_network = SmallNetwork(layer_dims, readoff_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_network_dataset[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(ideal_network_dataset[:1000]))):\n",
    "    [act.sum().item() for act in ideal_network_dataset[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([[act.sum().item() for act in acts][-1] for _, acts in ideal_network_dataset[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_network_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_network.layers[1].layer.weight.sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supercompute-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
