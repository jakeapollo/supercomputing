{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe just a bunch of ANDs in every layer, then ANDs of ANDs? Issue: sparsity goes down.\n",
    "\n",
    "Just replace ANDs with at-least-two-of-a-small-subset functions? Say each input node is on with prob\n",
    "$p$; consider an output node with $k$ inputs; the prob this has two inputs on is $p^2(1-p)^{k-2}\n",
    "\\binom{k}{2}$. The prob it has at least two inputs on is $1-(1-p)^k-kp (1-p)^{k-1}$. The difference\n",
    "between these two is $O\\left(\\frac{(pk)^3}{1-pk}\\right)$, from summing a geometric series. Setting\n",
    "the first one to $p$ gives $p^2(1-p)^{k-2} \\binom{k}{2}=p$, which implies\n",
    "$p(1-p)^{k-2}=\\frac{2}{k(k-1)}$. This should work out at about $p=\\frac{2}{(k-1)^2}$, up to\n",
    "lower-order corrections, which also makes the diff small. Equivalently, $k=\\sqrt{\\frac{2}{p}}+1$ We\n",
    "might want to correct the ideal network such that it is more precisely binary though? I.e., we might\n",
    "want to do it without ReLUs? I guess can try both options, but let's first try the one without ReLUs\n",
    "that just computes these gates with perfect binary outputs.\n",
    "\n",
    "\n",
    "Here's an alternative calculation in a slightly different setup (though probably they are the same\n",
    "up to error terms in some reasonable sense). Let's say each entry of the weight matrix Bernoulli\n",
    "with probability $q$ and each input is Bernoulli with probability $p$. We want it to be the case\n",
    "that taking a random matrix and a random input, the probability that an output is on is $p$. The\n",
    "probability it is on is the probability that there are at least two simultaneous hits from that row\n",
    "of the weight matrix and the input. Each hit has probability $pq$, so this has probability\n",
    "$1-(1-pq)^m-m pq (1-pq)^{m-1}$. So to keep sparsity constant, we want $1-(1-pq)^m-m pq\n",
    "(1-pq)^{m-1}=p$. Up to sth like a $O((mpq)^3/(1-mpq))$ term as before, we can just solve $p=(pq)^2\n",
    "m(m-1)/2$. This gives $q=\\sqrt{\\frac{2}{m(m-1)p}}$. A more precise solution can be found using\n",
    "numerical methods (after all, fixing $m$ and $p$, it's just a matter of finding a root of a\n",
    "polynomial in $q$), I think. But this should be fine for us for now.\n",
    "\n",
    "Jake: roughly we want $p=(qp)^2 m\\implies q=\\frac 1 {m\\sqrt p}$. If we decide to pick $p=q$, we have\n",
    "$p=m^{1/3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(x):\n",
    "    return F.relu(x) - F.relu(x - 1)\n",
    "\n",
    "\n",
    "def denoising_nonlinearity(x, epsilon):\n",
    "    x[torch.abs(x) < epsilon] = 0\n",
    "    return x\n",
    "\n",
    "\n",
    "# Custom layer with specified properties\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, probability_q, p_positive=0.5, bias_value=-1):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.probability_q = probability_q\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        # self.weights = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        # self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.layer = nn.Linear(input_dim, output_dim)\n",
    "        self.bias_value = bias_value\n",
    "        self.p_positive = p_positive\n",
    "        self.reset_parameters()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer.weight.data = torch.bernoulli(\n",
    "            torch.full((self.output_dim, self.input_dim), self.probability_q)\n",
    "        )\n",
    "        random_signs = (\n",
    "            torch.bernoulli(self.p_positive * torch.ones(self.output_dim, self.input_dim)) * 2 - 1\n",
    "        )\n",
    "        self.layer.weight.data = self.layer.weight.data * random_signs\n",
    "        self.layer.bias.data.fill_(self.bias_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return custom_activation(x)\n",
    "\n",
    "\n",
    "class SummationLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SummationLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Initialize coefficients as 1 or -1 with 50/50 probability\n",
    "        self.coefficients = torch.where(\n",
    "            torch.rand(input_dim) > 0.5, torch.tensor(1.0), torch.tensor(-1.0)\n",
    "        )\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust for the case where x might not be batched\n",
    "        if x.dim() == 1:\n",
    "            # x is a 1D tensor, implying a single sample rather than a batch\n",
    "            return torch.sum(x * self.coefficients, dim=0, keepdim=True)\n",
    "        else:\n",
    "            # x is a 2D tensor, implying a batch of samples\n",
    "            return torch.sum(x * self.coefficients, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "# Neural Network with L Custom Layers and a Summation Layer at the end\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, layer_dims, probability_q, p_positive=0.5, bias_value=-1, final_summation=False\n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(\n",
    "                CustomLayer(layer_dims[i - 1], layer_dims[i], probability_q, p_positive, bias_value)\n",
    "            )\n",
    "\n",
    "        # Add the summation layer at the end, treated as just another layer\n",
    "        self.final_summation = final_summation\n",
    "        if final_summation:\n",
    "            self.summation_layer = SummationLayer(layer_dims[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assert that none of the weights or biases are nan\n",
    "        for layer in self.layers:\n",
    "            assert not torch.isnan(layer.layer.weight).any(), f\"weights of layer {layer} are nan\"\n",
    "            assert not torch.isnan(layer.layer.bias).any(), f\"bias of layer {layer} are nan\"\n",
    "\n",
    "        # List to store activations from each layer, including the input and output layers\n",
    "        activations = [x]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            activations.append(x)  # Store the activation of each layer\n",
    "        # Apply the summation layer and treat its output as the activation of the final layer\n",
    "        if self.final_summation:\n",
    "            x = self.summation_layer(x)\n",
    "            activations.append(x)  # Include the final output as the last \"activation\"\n",
    "        return x, activations  # Return a list of activations for all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_with_unit_norm_columns(d, m):\n",
    "    \"\"\"\n",
    "    Create an d x m matrix E where each column has a unit norm.\n",
    "\n",
    "    Args:\n",
    "        d (int): Number of columns in W_E, corresponding to the dimension of V_1.\n",
    "        m (int): Number of rows in W_E, corresponding to the dimension of U_1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The matrix W_E with each row normalized to have a unit norm.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate a d x m matrix with Gaussian entries\n",
    "    W_E = torch.randn(m, d)\n",
    "\n",
    "    # Step 2: Normalize each column to have a unit norm\n",
    "    norms = torch.norm(W_E, dim=1, keepdim=True)\n",
    "    W_E_normalized = W_E / norms\n",
    "\n",
    "    return W_E_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many inputs do we need to give the small net a reasonable chance to learn the ideal algorithm? Well, for a lower bound, the number of times each output gate is active with one of the inputs being active should be at least one in expectation, I guess. But well, maybe this is a bad question to ask. We should probably just keep training on random inputs and track the loss over time. It'll plausibly be clear from the loss curve if the algo is still getting learned or if we've peaked. Should maybe still hold out a small number of inputs as a test set though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdealNetworkDataset(Dataset):\n",
    "    def __init__(self, length, m, p, W_E, ideal_network: CustomNetwork):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            length (int): Number of items in the dataset.\n",
    "            m (int): Dimension of the input vectors for the big network (U_1).\n",
    "            p (float): Probability of an entry being 1 in the input vector.\n",
    "            E (torch.Tensor): Matrix for mapping u to v (dimensions n x m).\n",
    "            ideal_network (CustomNetwork): The ideal (big) network to run input through and\n",
    "            get activations.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self.m = m\n",
    "        self.p = p\n",
    "        self.W_E = W_E\n",
    "        self.ideal_network = ideal_network\n",
    "        self.ideal_inputs = torch.bernoulli(torch.full((length, self.m), self.p))\n",
    "        self.compressed_inputs = self.ideal_inputs @ self.W_E\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a random vector u in U_1\n",
    "        ideal_input = self.ideal_inputs[idx]\n",
    "        compressed_input = self.compressed_inputs[idx]\n",
    "\n",
    "        # Run u (not v) through the ideal network and store activations at each layer\n",
    "        # This can be done for an entire batch at once.\n",
    "        _, ideal_activations = self.ideal_network(ideal_input)\n",
    "\n",
    "        # Return the input for the small network (v) and the corresponding activations as targets\n",
    "        return (compressed_input, ideal_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_dims,\n",
    "        readoff_dims,\n",
    "        nonlinearity=F.relu,\n",
    "        final_summation=False,\n",
    "        readoff_nonlinearity=denoising_nonlinearity,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the small network with specific layer dimensions and readoff layers for all\n",
    "        activations, including a separate readoff for the summation layer's output.\n",
    "\n",
    "        Args:\n",
    "        layer_dims (list): Dimensions of the small network's layers, including the input layer.\n",
    "            Typically [d]*L, where d is the dimension of the compressed input and L is the number of\n",
    "            layers.\n",
    "        readoff_dims (list): Target dimensions for the readoff layers, corresponding to each\n",
    "            layer in the big network. Typically [m]*L, where m is the dimension of the input to\n",
    "            the big network and L is the number of layers.\n",
    "        \"\"\"\n",
    "        super(SmallNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.readoff_layers = nn.ModuleList()\n",
    "\n",
    "        # Initialize the small network layers with ReLU activations\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(nn.Linear(layer_dims[i - 1], layer_dims[i]))\n",
    "\n",
    "        # Final summation layer\n",
    "        self.final_summation = final_summation\n",
    "        if final_summation:\n",
    "            self.summation_layer = SummationLayer(layer_dims[-1])\n",
    "\n",
    "        # Ensure we have a readoff layer for each layer in layer_dims plus one for the summation\n",
    "        # layer\n",
    "        for i, dim in enumerate(layer_dims):\n",
    "            self.readoff_layers.append(nn.Linear(dim, readoff_dims[i], bias=False))\n",
    "        if final_summation:\n",
    "            self.readoff_layers.append(nn.Linear(1, 1, bias=False))\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.readoff_nonlinearity = readoff_nonlinearity\n",
    "        # epsilons is a learnable list of thresholds for the denoising nonlinearity initialised at\n",
    "        # 1/sqrt(layer_dim) for each layer\n",
    "        self.epsilons = nn.ParameterList(\n",
    "            [nn.Parameter(torch.tensor(1 / np.sqrt(layer_dim))) for layer_dim in layer_dims]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [x]  # Store activations from the small network, including input\n",
    "        for layer in self.layers:\n",
    "            x = self.nonlinearity(layer(x))\n",
    "            activations.append(x)\n",
    "\n",
    "        # Apply the summation layer to the output of the last linear layer\n",
    "        if self.final_summation:\n",
    "            summation_output = self.summation_layer(activations[-1])\n",
    "            activations.append(summation_output)\n",
    "\n",
    "        # Map activations and summation output through their respective readoff layers\n",
    "        readoff_activations = []\n",
    "        for i, (readoff_layer, activation) in enumerate(zip(self.readoff_layers, activations)):\n",
    "            readoff_activation = readoff_layer(activation)\n",
    "            # print(\"activation: \", activation)\n",
    "            # print(\"readoff_activation: \", readoff_activation)\n",
    "            denoised_readoff = self.readoff_nonlinearity(readoff_activation, self.epsilons[i])\n",
    "            # print(\"denoised_readoff: \", denoised_readoff)\n",
    "            readoff_activations.append(denoised_readoff)\n",
    "\n",
    "        return activations, readoff_activations  # Adjust return types as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighted_MSE(predictions: torch.Tensor, targets: torch.Tensor, ones_weighting=1):\n",
    "    \"\"\"\n",
    "    Compute MSE between predictions and targets. Predictions should be a batch of vectors with\n",
    "    entries that are zero and one. The MSE is reweighted to give more importance to the ones in the\n",
    "    targets, so that for each sample, the MSE is computed as:\n",
    "    filter the predictions by targets that should be 1. Then compute MSE and divide by the number of\n",
    "    1s in the targets. Then multiply by ones_weighting.\n",
    "    Then filter the predictions by targets that should be 0. Compute MSE and divide by the number of\n",
    "    0s in the targets.\n",
    "    Then sum the two MSEs.\n",
    "    \"\"\"\n",
    "    assert predictions.shape == targets.shape\n",
    "    assert torch.all((targets == 0) | (targets == 1))\n",
    "    assert not torch.isnan(predictions).any(), \"Predictions contain NaNs\"\n",
    "    assert not torch.isnan(targets).any(), \"Targets contain NaNs\"\n",
    "    ones_predictions = predictions * targets\n",
    "    ones_mse = torch.sum((ones_predictions - targets) ** 2, dim=-1) / torch.sum(targets, dim=-1)\n",
    "    ones_mse[targets.sum(dim=-1) == 0] = 0  # Avoid NaNs when there are no ones in the target\n",
    "    zeros_predictions = predictions * (1 - targets)\n",
    "    zeros_mse = torch.sum(zeros_predictions**2, dim=-1) / torch.sum(1 - targets, dim=-1)\n",
    "    zeros_mse[targets.sum(dim=-1) == targets.shape[-1]] = (\n",
    "        0  # Avoid NaNs when there are no zeros in the target\n",
    "    )\n",
    "    total_MSE = ones_weighting * ones_mse + zeros_mse\n",
    "    return total_MSE.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(small_network, dataloader, lr=1e-3, epsilon_loss_weight=0.0):\n",
    "    performance_criterion = reweighted_MSE\n",
    "    optimizer = torch.optim.Adam(small_network.parameters(), lr=lr)\n",
    "\n",
    "    for step, (compressed_input, ideal_activations) in enumerate(tqdm(dataloader)):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # assert no nans in ideal_activations or compressed_input\n",
    "        ideal_activations = [activation.float() for activation in ideal_activations]\n",
    "        compressed_input = compressed_input.float()\n",
    "        assert not torch.isnan(compressed_input).any(), \"Compressed input contains NaNs\"\n",
    "        assert not torch.isnan(ideal_activations[0]).any(), \"Ideal activation contains NaNs\"\n",
    "\n",
    "        small_activations, readoff_activations = small_network(compressed_input)\n",
    "        assert not torch.isnan(small_activations[0]).any(), \"Small activation contains NaNs\"\n",
    "        assert all(\n",
    "            [not torch.isnan(readoff_activation).any()]\n",
    "            for readoff_activation in readoff_activations\n",
    "        ), \"Readoff activation contains NaNs\"\n",
    "\n",
    "        # Compute loss\n",
    "        total_loss = 0\n",
    "        for layer in range(len(ideal_activations)):\n",
    "            readoff_activation = readoff_activations[layer]\n",
    "            ideal_activation = ideal_activations[layer]\n",
    "            assert not torch.isnan(\n",
    "                readoff_activation\n",
    "            ).any(), \"Readoff activation contains NaNs at layer \" + str(layer)\n",
    "            assert not torch.isnan(ideal_activation).any(), \"Ideal activation contains NaNs\"\n",
    "            performance_loss = performance_criterion(readoff_activation, ideal_activation)\n",
    "            epsilons = small_network.epsilons[layer]\n",
    "            epsilon_penalty = torch.norm(epsilons, p=4)\n",
    "            total_loss += performance_loss + epsilon_penalty * epsilon_loss_weight\n",
    "            # print everything and break out of the entire training loop if total_loss is nan\n",
    "            if torch.isnan(total_loss):\n",
    "                print(\"Performance loss: \", performance_loss)\n",
    "                print(\"Epsilon penalty: \", epsilon_penalty)\n",
    "                print(\"Total loss: \", total_loss)\n",
    "                raise ValueError(\"Total loss is NaN\")\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        # if step % 1 == 0:\n",
    "        print(\"Loss: \", total_loss.item())\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000  # dim of ideal sparse net\n",
    "# prob each input is on; should also be the prob each gate later on is on\n",
    "p = 1 / 100  # math.log(m) * m ** (-1)\n",
    "q = 0.1  # (2 / (m * (m - 1) * p)) ** (0.1)  # prob each weight matrix entry is nonzero\n",
    "print(p, q)\n",
    "b = -1\n",
    "n = 100  # dim into which we'll try to compress the ideal net\n",
    "dataset_length = 10000  # Number of data points\n",
    "batch_size = 50  # Batch size for training\n",
    "L = 8  # num of layers, including input but not the 1-neuron output\n",
    "layer_dims = [m] * L  # Dimension of each layer including input and output dimension\n",
    "probability_q = q  # Probability of presence of each entry in weight matrix\n",
    "# k = math.sqrt(2/p)+1 # fan-in\n",
    "\n",
    "p_positive = 0.25  # Probability of a weight being positive\n",
    "ideal_network = CustomNetwork(layer_dims, probability_q, p_positive, bias_value=b)\n",
    "\n",
    "# Initialize the dataset\n",
    "W_E = create_matrix_with_unit_norm_columns(n, m)\n",
    "\n",
    "ideal_network_dataset = IdealNetworkDataset(dataset_length, m, p, W_E, ideal_network)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(ideal_network_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "layer_dims = [n] * L  # Layer dimensions for the small network, excluding the summation layer\n",
    "readoff_dims = [m] * L  # Readoff dimensions for each layer plus the summation layer\n",
    "\n",
    "small_network = SmallNetwork(layer_dims, readoff_dims)\n",
    "\n",
    "num_workers = 1  # Number of workers for the DataLoader\n",
    "dataloader = DataLoader(\n",
    "    ideal_network_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(small_network, dataloader, lr=1e-3, epsilon_loss_weight=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = ideal_network_dataset[:][1]\n",
    "plt.hist([act.sum(dim=-1) for act in acts][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_network.epsilons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supercompute-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
